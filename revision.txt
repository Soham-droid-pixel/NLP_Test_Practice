Perfect ğŸ‘
Iâ€™ll give you a **super clean list** of the **libraries + when to use them** for your NLP exam (1â€“13 problems).

---

## **ğŸ“Œ NLP Exam Library Cheat Sheet (1â€“13 Problems)**

---

### **1ï¸âƒ£ `re` (Regular Expressions)**

* **When to use:** Extract emails, phone numbers, URLs, hashtags, mentions, dates, offensive words.
* **Key functions:**

  * `re.findall(pattern, text)` â†’ Extract matches
  * `re.sub(pattern, repl, text)` â†’ Replace/remove unwanted parts

---

### **2ï¸âƒ£ `nltk` (Natural Language Toolkit)**

* **When to use:**

  * Tokenization (word/sentence)
  * N-Grams (unigrams, bigrams, trigrams)
  * Stopword removal
  * POS tagging
  * Stemming / Lemmatization
  * Edit distance
  * Chunking
  * HMM POS Tagging
* **Key functions:**

  * `nltk.word_tokenize(text)`
  * `nltk.pos_tag(tokens)`
  * `ngrams(tokens, n)`
  * `stopwords.words('english')`
  * `PorterStemmer().stem(word)` / `LancasterStemmer().stem(word)`
  * `WordNetLemmatizer().lemmatize(word)`
  * `edit_distance(word1, word2)`
  * `RegexpParser(grammar).parse(tags)`

---

### **3ï¸âƒ£ `spacy`**

* **When to use:**

  * POS tagging (Universal POS tags)
  * Named Entity Recognition (NER)
  * Stopword removal (token.is\_stop)
  * Tokenization (token.text)
* **Key functions:**

  * `spacy.load("en_core_web_sm")`
  * `doc = nlp(text)`
  * `token.pos_`, `token.is_stop`
  * `doc.ents` (ent.text, ent.label\_)

---

### **4ï¸âƒ£ `transformers` (HuggingFace)**

* **When to use:**

  * BERT tokenization (subword tokens)
  * POS / NER with pretrained models
* **Key functions:**

  * `BertTokenizer.from_pretrained("bert-base-uncased")`
  * `.tokenize(text)`
  * `pipeline("token-classification", model="dslim/bert-base-NER")`

---

### **5ï¸âƒ£ `string`**

* **When to use:** Remove punctuation during text cleaning.
* **Key functions:**

  * `string.punctuation` â†’ all punctuation symbols

---

### **6ï¸âƒ£ `nltk.corpus.brown` & `nltk.tag.hmm`**

* **When to use:** Train Hidden Markov Model (HMM) POS Tagger.
* **Key functions:**

  * `brown.tagged_sents()`
  * `HiddenMarkovModelTrainer().train_supervised()`

---

ğŸ’¡ **Memory Trick**

* **Regex (re)** â†’ Pattern finding (emails, phones, hashtags)
* **NLTK** â†’ *Basics of NLP* (tokenizing, stopwords, POS, stemming, lemmatization, edit distance, chunking)
* **SpaCy** â†’ *Advanced NLP* (POS Universal tags, NER, stopword flags)
* **Transformers** â†’ *Deep Learning NLP* (BERT subwords, NER)
* **String** â†’ Remove punctuation
* **Brown/HMM** â†’ Statistical POS tagging

---

ğŸ‘‰ Question for you:
Do you want me to also make a **problem â†’ library map** (like *Problem 1: Regex â†’ re*, *Problem 2: N-grams â†’ nltk*, â€¦) so you instantly know which library to call for each question? That will make exam navigation much faster.
Perfect âœ… You basically want a **master cheat sheet** for all NLP lab problems so you can recall **logic + syntax quickly** without re-reading everything.

Hereâ€™s a **compact but complete revision pack**:

---

# ğŸ“œ **ğŸ”¥ NLP PRACTICAL REVISION NOTES (Quick Recall + Syntax Tricks)**

---

## **1ï¸âƒ£ REGEX â€“ Information Extraction**

**Tasks:** Email, Phone, URL, Hashtag, Mention, Offensive Words

**Memory Trick:**
ğŸ‘‰ *Think â€œEP-UHMâ€ â†’ Email, Phone, URL, Hashtag, Mention*

**Syntax:**

```python
import re
text = """..."""  

emails   = re.findall(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", text)
phones   = re.findall(r"\+91-\d{10}", text)
urls     = re.findall(r"https?://\S+", text)
hashtags = re.findall(r"#\w+", text)
mentions = re.findall(r"@\w+", text)
offensive_words = [w for w in ["badword1","badword2","spamword"] if w in text]
```

âœ… **Tip:** All patterns are `re.findall(r"PATTERN", text)`.

---

## **2ï¸âƒ£ N-GRAMS â€“ Context Capture**

**Memory Trick:**
ğŸ‘‰ *Ngrams â†’ n=1,2,3 = Unigrams, Bigrams, Trigrams*

**Syntax:**

```python
from nltk.util import ngrams
import nltk; nltk.download('punkt')
tokens = nltk.word_tokenize(text)
list(ngrams(tokens, 1))  # Uni
list(ngrams(tokens, 2))  # Bi
list(ngrams(tokens, 3))  # Tri
```

âœ… **Tip:** Bigger n = more context.

---

## **3ï¸âƒ£ EDIT DISTANCE (Levenshtein)**

**Memory Trick:**
ğŸ‘‰ `edit_distance(w1,w2)` from `nltk.metrics`

**Syntax:**

```python
from nltk.metrics import edit_distance
edit_distance("kitten","sitting")  # â†’ 3
```

---

## **4ï¸âƒ£ CHUNKING â€“ Noun Phrases**

**Memory Trick:**
ğŸ‘‰ *Grammar `"NP: {<DT>?<JJ>*<NNP>+}"` â†’ DT optional, JJ adjectives, NNP proper noun*

**Syntax:**

```python
grammar = "NP: {<DT>?<JJ>*<NNP>+}"
chunk_parser = nltk.RegexpParser(grammar)
tree = chunk_parser.parse(nltk.pos_tag(nltk.word_tokenize(sentence)))
tree.draw()
```

---

## **5ï¸âƒ£ NER â€“ Named Entities**

**Memory Trick:**
ğŸ‘‰ *SpaCy â†’ `for ent in nlp(text).ents: print(ent.text, ent.label_)`*

**Syntax:**

```python
import spacy
nlp = spacy.load("en_core_web_sm")
for ent in nlp(text).ents:
    print(ent.text, ent.label_)
```

---

## **6ï¸âƒ£ TOKENIZATION â€“ Word / Char / Subword**

**Memory Trick:**
ğŸ‘‰ `NLTK=word_tokenize | SpaCy=token.text | BERT=tokenizer.tokenize`

**Syntax:**

```python
nltk.word_tokenize(text)            # Word (NLTK)
[token.text for token in nlp(text)] # Word (SpaCy)
list(text)                          # Char
tokenizer.tokenize(text)            # Subword
```

---

## **7ï¸âƒ£ STOPWORD REMOVAL**

**Memory Trick:**
ğŸ‘‰ *NLTK â†’ T-S-F (Tokenize, Stopwords, Filter)*
ğŸ‘‰ *SpaCy â†’ N-F (NLP, Filter token.is\_stop)*

**Syntax:**

```python
# NLTK
nltk_filtered = [w for w in nltk.word_tokenize(text) if w.lower() not in stopwords.words('english')]
# SpaCy
spacy_filtered = [t.text for t in nlp(text) if not t.is_stop]
```

---

## **8ï¸âƒ£ STEMMING vs LEMMATIZATION**

**Memory Trick:**
ğŸ‘‰ *Porter = light cut, Lancaster = aggressive cut, Lemma = correct word*

**Syntax:**

```python
porter.stem(w); lancaster.stem(w); lemmatizer.lemmatize(w)
```

---

## **9ï¸âƒ£ NOISE REMOVAL**

**Memory Trick:**
ğŸ‘‰ *Lower â†’ Remove symbols â†’ Extra spaces*

**Syntax:**

```python
text = text.lower()
text = re.sub(r"@\w+|#\w+|http\S+|[^a-zA-Z\s]", "", text)
text = re.sub(r"\s+", " ", text).strip()
```

---

## **ğŸ”Ÿ POS TAGGING â€“ NLTK vs SpaCy**

**Memory Trick:**
ğŸ‘‰ *NLTK â†’ pos\_tag(tokenize) â†’ Penn tags
SpaCy â†’ token.pos\_ â†’ Universal tags*

**Syntax:**

```python
# NLTK
nltk.pos_tag(nltk.word_tokenize(text))
# SpaCy
[(t.text, t.pos_) for t in nlp(text)]
```

---

## **1ï¸âƒ£1ï¸âƒ£ RULE-BASED POS (RegexpTagger)**

**Memory Trick:**
ğŸ‘‰ *Patterns list â†’ RegexpTagger(patterns).tag(sentence)*

**Syntax:**

```python
patterns=[(r'.*ing$','VBG'), (r'^Ravi$','NNP'), (r'.*','NN')]
RegexpTagger(patterns).tag(words)
```

---

## **1ï¸âƒ£2ï¸âƒ£ HMM POS TAGGER**

**Memory Trick:**
ğŸ‘‰ *Brown corpus â†’ train\_supervised â†’ tag(sentence)*

**Syntax:**

```python
from nltk.corpus import brown
from nltk.tag import hmm
train = brown.tagged_sents(categories='news', tagset='universal')[:500]
tagger = hmm.HiddenMarkovModelTrainer().train_supervised(train)
tagger.tag(sentence.split())
```

---

## **1ï¸âƒ£3ï¸âƒ£ BERT POS / NER**

**Memory Trick:**
ğŸ‘‰ *HuggingFace pipeline(â€œtoken-classificationâ€)*

**Syntax:**

```python
from transformers import pipeline
nlp_pipe = pipeline("token-classification", model="dslim/bert-base-NER")
nlp_pipe(text)
```

---

# ğŸ¯ **How to Memorize All This**

* **Group by topic** (Regex, Tokenization, Stopwords, POS, NER)
* **Remember just key functions** per library:

  * **NLTK:** `word_tokenize`, `pos_tag`, `RegexpParser`, `edit_distance`, `stopwords`, `stem()`
  * **SpaCy:** `nlp(text)`, `token.text`, `token.pos_`, `token.is_stop`
  * **HuggingFace:** `pipeline("token-classification")`
* **Practice once** in VS Code â†’ after running, focus on **remembering function names** (syntax auto-completes in exam).

---

ğŸ‘‰ I can also make this into a **one-page â€œVisual Cheat Sheetâ€** (tables + arrows for each task).
That will help you recall all syntax instantly.

Do you want me to create that one-page diagram for quick exam revision? (It will be much easier than reading all text.)
